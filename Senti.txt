from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch
import os
os.environ["WANDB_DISABLED"] = "true"
import pandas as pd
from datasets import Dataset

# Load your custom CSV
df = pd.read_csv("/content/drive/MyDrive/sem8/NLP/sentiment.csv",encoding='latin1')  
df = df.dropna(subset=["text", "sentiment"])

label_map = {"negative": 0, "neutral": 1, "positive": 2}  
df["sentiment"] = df["sentiment"].map(label_map) 
df = df.rename(columns={"text": "text", "sentiment": "label"})
df["text"] = df["text"].astype(str)
dataset = Dataset.from_pandas(df)

dataset = dataset.train_test_split(test_size=0.1)

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="no",
    num_train_epochs=0.5,  # quick test
    per_device_train_batch_size=4,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    load_best_model_at_end=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].select(range(200)),
    eval_dataset=tokenized_datasets["test"].select(range(100)),
    tokenizer=tokenizer,
)

trainer.train()

# Save the fine-tuned model
trainer.save_model("./results")  # Save for later use
results = trainer.evaluate()
print("Evaluation results:", results)

# Load the fine-tuned model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("./results")  # load trained model

# Inference: Predict sentiment of a new review
def predict_sentiment(text):
    model.eval() 

    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True)

    # Get model's output (logits)
    with torch.no_grad():  # avoid tracking gradients
        outputs = model(**inputs)

    # Get predicted probabilities (softmax)
    logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=-1)

    # Get predicted label (0 = negative, 1 = positive)
    prediction = torch.argmax(probs, dim=-1).item()

    # Return prediction
    return "positive" if prediction == 1 else "negative"

test_texts = [
    "The movie was fantastic! I really loved it.",
    "It was the worst movie I have ever seen, so boring.",
    "A wonderful experience with great acting and storyline.",
    "Terrible! I would not recommend it to anyone."
]

for text in test_texts:
    sentiment = predict_sentiment(text)
    print(f"Review: {text}")
    print(f"Predicted Sentiment: {sentiment}\n")





#now for test and train 
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import pandas as pd
import torch

# Load train and test CSVs
train_df = pd.read_csv("NLP/train.csv", encoding='latin1')
test_df = pd.read_csv("NLP/test.csv", encoding='latin1')

# Clean and map sentiment labels
label_map = {"negative": 0, "neutral": 1, "positive": 2}
for df in [train_df, test_df]:
    df.dropna(subset=["text", "sentiment"], inplace=True)
    df["sentiment"] = df["sentiment"].map(label_map)
    df["text"] = df["text"].astype(str)
    df.rename(columns={"sentiment": "label"}, inplace=True)

# Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Tokenizer and Model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)

def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="no",
    num_train_epochs=0.5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    load_best_model_at_end=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset.select(range(min(200, len(train_dataset)))),  # limit size for test
    eval_dataset=test_dataset.select(range(min(100, len(test_dataset)))),
    tokenizer=tokenizer,
)

trainer.train()

trainer.save_model("./results")

results = trainer.evaluate()
print("Evaluation results:", results)
