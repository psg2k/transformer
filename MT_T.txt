!pip install datasets
import pandas as pd
from datasets import Dataset
from transformers import (
    MarianTokenizer,
    MarianMTModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)

model_name = "Helsinki-NLP/opus-mt-en-fr"
batch_size = 4
epochs = 3

# Load dataset (CSV with 'source' and 'target' columns)
df=pd.read_csv("/content/drive/MyDrive/sem8/NLP/eng_-french.csv",encoding='latin1')
print(df.columns)
df.rename(columns={
    'English words/sentences': 'source',
    'French words/sentences': 'target',
}, inplace=True)
df = df.sample(n=100, random_state=42).reset_index(drop=True)

dataset = Dataset.from_pandas(df)

tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

def preprocess(example):
    model_inputs = tokenizer(example["source"], max_length=128, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess, batched=True)

training_args = Seq2SeqTrainingArguments(
    output_dir="./tmp",
    per_device_train_batch_size=batch_size,
    num_train_epochs=epochs,
    report_to="none"
)

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)
trainer.train()

def translate(text):
    encoded = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    generated = model.generate(**encoded)
    return tokenizer.decode(generated[0], skip_special_tokens=True)

print(translate("How are you?"))
